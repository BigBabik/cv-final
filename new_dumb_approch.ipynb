{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from copy import deepcopy\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import torch\n",
    "import pandas as pd\n",
    "from models.superglue import SuperGlue\n",
    "from models.superpoint import SuperPoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_config = {\n",
    "    'superpoint': {\n",
    "    'descriptor_dim': 256,\n",
    "    'nms_radius': 4,\n",
    "    'keypoint_threshold': 0.005,\n",
    "    'max_keypoints': -1,\n",
    "    'remove_borders': 4,\n",
    "    },\n",
    "    'superglue': {\n",
    "    'descriptor_dim': 256,\n",
    "    'weights': 'outdoor',\n",
    "    'keypoint_encoder': [32, 64, 128, 256],\n",
    "    'GNN_layers': ['self', 'cross'] * 9,\n",
    "    'sinkhorn_iterations': 100,\n",
    "    'match_threshold': 0.2,\n",
    "    'max_keypoints': -1 \n",
    "    }\n",
    "}\n",
    "\n",
    "best_superglue_config = {\n",
    "    'descriptor_dim': 256,\n",
    "    'weights': 'outdoor',\n",
    "    'keypoint_encoder': [32, 64, 128, 256],\n",
    "    'GNN_layers': ['self', 'cross'] * 9,\n",
    "    'sinkhorn_iterations': 100,\n",
    "    'match_threshold': 0.2,\n",
    "    'max_keypoints': -1 \n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TensorFromCVKps(kps):\n",
    "    keypoint_data = []\n",
    "    for kp in kps:\n",
    "        # Each keypoint is represented as (x, y, size, response)\n",
    "        keypoint_data.append((kp.pt[0], kp.pt[1]))\n",
    "    kypoints_tensor = torch.tensor(keypoint_data)\n",
    "    return kypoints_tensor\n",
    "\n",
    "def ArrayFromCvKps(kps):\n",
    "    '''Convenience function to convert OpenCV keypoints into a simple numpy array.'''\n",
    "    \n",
    "    return np.array([kp.pt for kp in kps])\n",
    "\n",
    "def ExtractRootSiftFeatures(image, detector, num_features):\n",
    "    '''Compute RootSIFT features for a given image.'''\n",
    "    # Convert the image to grayscale\n",
    "\n",
    "    gray = cv2.cvtColor(cv2.imread(image), cv2.COLOR_RGB2GRAY)\n",
    "    \n",
    "    # Detect SIFT keypoints and descriptors\n",
    "    keypoints, desc = detector.detectAndCompute(gray, None)\n",
    "    \n",
    "    # Apply RootSIFT normalization if descriptors exist\n",
    "    if desc is not None:\n",
    "        desc /= (np.linalg.norm(desc, axis=1, keepdims=True) + 1e-7)\n",
    "        desc = np.sqrt(desc)\n",
    "\n",
    "    \n",
    "    responses = np.array([kp.response for kp in keypoints])\n",
    "    \n",
    "    # Normalize the responses using min-max scaling\n",
    "    scores = (responses - responses.min()) / (responses.max() - responses.min())\n",
    "\n",
    "    return keypoints[:num_features], desc[:num_features] if desc is not None else [], scores\n",
    "\n",
    "def get_string_fundamental_matrix(cur_kp_1, cur_kp_2):\n",
    "    F, inlier_mask = cv2.findFundamentalMat(\n",
    "        cur_kp_1,\n",
    "        cur_kp_2, \n",
    "        cv2.USAC_MAGSAC, \n",
    "        ransacReprojThreshold=0.5,\n",
    "        confidence=0.99999,\n",
    "        maxIters=10000)\n",
    "    \n",
    "    F = np.array(F)\n",
    "    if F is None:\n",
    "        F = np.zeros(9)\n",
    "    if F.shape != (3,3):\n",
    "        print(F.shape)\n",
    "    F = F.reshape(-1)\n",
    "    F = F[:9]\n",
    "\n",
    "    F_string_format = \" \".join(f\"{num:.5e}\" for num in F)\n",
    "\n",
    "    return F_string_format\n",
    "\n",
    "\n",
    "def find_F_for_2_images_sift_bf(image0_path, image1_path, sift_detector, bf):\n",
    "    \n",
    "    keypoints_1, descriptors_1, scores1 = ExtractRootSiftFeatures(image0_path, sift_detector, 2000)\n",
    "    keypoints_2, descriptors_2, scores2 = ExtractRootSiftFeatures(image1_path, sift_detector, 2000)\n",
    "    \n",
    "    cv_matches = bf.match(descriptors_1, descriptors_2)\n",
    "    matches = np.array([[m.queryIdx, m.trainIdx] for m in cv_matches])\n",
    "    cur_kp_1 = ArrayFromCvKps([keypoints_1[m[0]] for m in matches])\n",
    "    cur_kp_2 = ArrayFromCvKps([keypoints_2[m[1]] for m in matches])\n",
    "\n",
    "    F, inlier_mask = cv2.findFundamentalMat(\n",
    "        cur_kp_1,\n",
    "        cur_kp_2, \n",
    "        cv2.USAC_MAGSAC, \n",
    "        ransacReprojThreshold=0.5,\n",
    "        confidence=0.99999,\n",
    "        maxIters=10000)\n",
    "    \n",
    "    F = np.array(F)\n",
    "    if F is None:\n",
    "        F = np.zeros(9)\n",
    "    if F.shape != (3,3):\n",
    "        print(F.shape)\n",
    "    F = F.reshape(-1)\n",
    "    F = F[:9]\n",
    "\n",
    "    F_string_format = \" \".join(f\"{num:.5e}\" for num in F)\n",
    "\n",
    "    return F_string_format\n",
    "\n",
    "\n",
    "def create_sample_id_data(image0_name, image1_name, scene_name, F_string):\n",
    "    sample_id = f\"{scene_name};{image0_name}-{image1_name}\"\n",
    "    data_row = {\"sample_id\": sample_id, \"fundamental_matrix\": F_string}\n",
    "    return data_row\n",
    "\n",
    "def frame2tensor(frame, device):\n",
    "    return torch.from_numpy(frame/255.).float()[None, None].to(device)\n",
    "\n",
    "def find_F_sift_superglue(image0_path, image1_path, sift_detector, super_glue):\n",
    "    keypoints_1, descriptors_1, scores1 = ExtractRootSiftFeatures(image0_path, sift_detector, 2000)\n",
    "    keypoints_2, descriptors_2, scores2 = ExtractRootSiftFeatures(image1_path, sift_detector, 2000)\n",
    "\n",
    "    gray1 = cv2.cvtColor(cv2.imread(image0_path), cv2.COLOR_RGB2GRAY)\n",
    "    gray2 = cv2.cvtColor(cv2.imread(image1_path), cv2.COLOR_RGB2GRAY)\n",
    "    image1_data, image2_data = frame2tensor(gray1, get_device()), frame2tensor(gray2, get_device())\n",
    "\n",
    "    \n",
    "    data = {}\n",
    "    data['keypoints0'], data['keypoints1'] = TensorFromCVKps(keypoints_1), TensorFromCVKps(keypoints_2)\n",
    "    data['descriptors0'], data['descriptors1'] = torch.tensor(descriptors_1), torch.tensor(descriptors_2)\n",
    "    data['image0'], data['image1'] = image1_data, image2_data\n",
    "    data['scores0'], data['scores1'] = torch.tensor(scores1), torch.tensor(scores2)\n",
    "\n",
    "    pred = super_glue(data)\n",
    "    pred = {k: v[0].cpu().numpy() for k, v in pred.items()}\n",
    "    kpts0, kpts1 = pred['keypoints0'], pred['keypoints1']\n",
    "    matches, match_conf = pred['matches0'], pred['matching_scores0']\n",
    "\n",
    "    # Write the matches to disk.\n",
    "    out_matches = {'keypoints0': kpts0, 'keypoints1': kpts1,    ### Edited - write to disc\n",
    "                    'matches': matches, 'match_confidence': match_conf }\n",
    "    #np.savez(str(matches_path), **out_matches)\n",
    "\n",
    "    AMOUNT = 10\n",
    "    THRESH = 0.95\n",
    "    while cnt < AMOUNT and THRESH >= 0.1:\n",
    "        mkpts0 = []\n",
    "        mkpts1 = []\n",
    "        cnt = 0\n",
    "        for i in range(len(matches)):\n",
    "            is_valid = matches[i] > -1 and match_conf[i] >= THRESH\n",
    "            if is_valid:\n",
    "                cnt += 1\n",
    "                mkpts0.append(kpts0[i])\n",
    "                mkpts1.append(kpts1[matches[i]])\n",
    "        THRESH -= 0.05\n",
    "\n",
    "    mkpts0 = np.array(mkpts0)\n",
    "    mkpts1 = np.array(mkpts1)\n",
    "\n",
    "    F_string = get_string_fundamental_matrix(mkpts0, mkpts1)\n",
    "\n",
    "    return F_string\n",
    "\n",
    "\n",
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        device = 'cuda'\n",
    "    else:\n",
    "        device = 'cpu'\n",
    "    return device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/app/models/superglue.py:227: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(str(path)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded SuperGlue model (\"outdoor\" weights)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/18840 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[torch.Size([1, 2, 2000]), torch.Size([5000, 1])]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Tensors must have same number of dimensions: got 3 and 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m image0_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbase_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscene_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimage0_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     16\u001b[0m image1_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbase_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscene_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimage1_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 18\u001b[0m F_string \u001b[38;5;241m=\u001b[39m \u001b[43mfind_F_sift_superglue\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage0_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage1_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msift_detector\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msuper_glue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m img_data \u001b[38;5;241m=\u001b[39m create_sample_id_data(image0_name, image1_name, scene_name, F_string)\n\u001b[1;32m     20\u001b[0m img_data_list\u001b[38;5;241m.\u001b[39mappend(img_data)\n",
      "Cell \u001b[0;32mIn[3], line 112\u001b[0m, in \u001b[0;36mfind_F_sift_superglue\u001b[0;34m(image0_path, image1_path, sift_detector, super_glue)\u001b[0m\n\u001b[1;32m    109\u001b[0m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage0\u001b[39m\u001b[38;5;124m'\u001b[39m], data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage1\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m image1_data, image2_data\n\u001b[1;32m    110\u001b[0m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscores0\u001b[39m\u001b[38;5;124m'\u001b[39m], data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscores1\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(scores1), torch\u001b[38;5;241m.\u001b[39mtensor(scores2)\n\u001b[0;32m--> 112\u001b[0m pred \u001b[38;5;241m=\u001b[39m \u001b[43msuper_glue\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    113\u001b[0m pred \u001b[38;5;241m=\u001b[39m {k: v[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy() \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m pred\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m    114\u001b[0m kpts0, kpts1 \u001b[38;5;241m=\u001b[39m pred[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkeypoints0\u001b[39m\u001b[38;5;124m'\u001b[39m], pred[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkeypoints1\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/app/models/superglue.py:249\u001b[0m, in \u001b[0;36mSuperGlue.forward\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    246\u001b[0m kpts1 \u001b[38;5;241m=\u001b[39m normalize_keypoints(kpts1, data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage1\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# Keypoint MLP encoder.\u001b[39;00m\n\u001b[0;32m--> 249\u001b[0m desc0 \u001b[38;5;241m=\u001b[39m desc0 \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkenc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkpts0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mscores0\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    250\u001b[0m desc1 \u001b[38;5;241m=\u001b[39m desc1 \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkenc(kpts1, data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscores1\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m    252\u001b[0m \u001b[38;5;66;03m# Multi-layer Transformer network.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/app/models/superglue.py:85\u001b[0m, in \u001b[0;36mKeypointEncoder.forward\u001b[0;34m(self, kpts, scores)\u001b[0m\n\u001b[1;32m     83\u001b[0m inputs \u001b[38;5;241m=\u001b[39m [kpts\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m), scores\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)]\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28mprint\u001b[39m([x\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m inputs])\n\u001b[0;32m---> 85\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Tensors must have same number of dimensions: got 3 and 2"
     ]
    }
   ],
   "source": [
    "#### Sift Root + super Glue\n",
    "import pandas as pd\n",
    "input_csv = \"/share/project_data/test.csv\"\n",
    "base_path = \"/share/project_data/test_images\"\n",
    "test_samples = pd.read_csv(input_csv)\n",
    "img_data_list = []\n",
    "num_features = 5000\n",
    "sift_detector = cv2.SIFT_create(num_features, contrastThreshold=-10000, edgeThreshold=-10000)\n",
    "super_glue = SuperGlue(best_superglue_config).eval().to(get_device())\n",
    "\n",
    "\n",
    "for index, row in tqdm(test_samples.iterrows(), total=len(test_samples)):\n",
    "    image0_name, image1_name  = row['image_1_id'], row['image_2_id']\n",
    "    scene_name = row['batch_id']\n",
    "    image0_path = f\"{base_path}/{scene_name}/{image0_name}.jpg\"\n",
    "    image1_path = f\"{base_path}/{scene_name}/{image1_name}.jpg\"\n",
    "\n",
    "    F_string = find_F_sift_superglue(image0_path, image1_path, sift_detector, super_glue)\n",
    "    img_data = create_sample_id_data(image0_name, image1_name, scene_name, F_string)\n",
    "    img_data_list.append(img_data)\n",
    "\n",
    "output = pd.DataFrame(img_data_list)\n",
    "output.to_csv(\"/share/project_data/submit_result_bsift_super_glue.csv\", index=False)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5000])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_features = 5000\n",
    "gray1 = cv2.cvtColor(cv2.imread(image1_path), cv2.COLOR_RGB2GRAY)\n",
    "image1_data = frame2tensor(gray1, get_device())\n",
    "sift_detector = cv2.SIFT_create(num_features, contrastThreshold=-10000, edgeThreshold=-10000)\n",
    "keypoints_1, descriptors_1, scores = ExtractRootSiftFeatures(\"/share/project_data/test_images/trevi_fountain/00644051_242819650.jpg\", sift_detector, 10000)\n",
    "torch.tensor(scores).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/app/models/superpoint.py:137: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(str(path)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded SuperPoint model\n"
     ]
    }
   ],
   "source": [
    "def create_super_point():\n",
    "    default_config = {\n",
    "        'descriptor_dim': 256,\n",
    "        'nms_radius': 4,\n",
    "        'keypoint_threshold': 0.005,\n",
    "        'max_keypoints': -1,\n",
    "        'remove_borders': 4,\n",
    "    }\n",
    "    super_point = SuperPoint(default_config)\n",
    "    return super_point\n",
    "\n",
    "\n",
    "def extract_feature_super_point(image_path, super_point):\n",
    "    \n",
    "    gray = cv2.cvtColor(cv2.imread(image_path), cv2.COLOR_RGB2GRAY)\n",
    "    gray = gray.astype('float32')\n",
    "    image1_data = frame2tensor(gray, 'cpu')\n",
    "    pred = super_point({'image': image1_data})\n",
    "    return pred                   \n",
    "\n",
    "\n",
    "pred = extract_feature_super_point(\"/share/project_data/test_images/trevi_fountain/00644051_242819650.jpg\", create_super_point())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_365/1824166556.py:1: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments. To learn more, see the migration guide https://numpy.org/devdocs/numpy_2_0_migration_guide.html#adapting-to-changes-in-the-copy-keyword\n",
      "  np.array(pred[\"scores\"][0]).unsqueeze(1).shape\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mscores\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mshape\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py:1149\u001b[0m, in \u001b[0;36mTensor.__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m   1147\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(Tensor\u001b[38;5;241m.\u001b[39m__array__, (\u001b[38;5;28mself\u001b[39m,), \u001b[38;5;28mself\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m   1148\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1149\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1150\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead."
     ]
    }
   ],
   "source": [
    "pred[\"scores\"][0].unsqueeze(1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'keypoints0': [tensor([[ 339.,    8.],\n",
       "          [ 425.,    8.],\n",
       "          [ 713.,    8.],\n",
       "          ...,\n",
       "          [ 529., 1055.],\n",
       "          [ 643., 1055.],\n",
       "          [ 711., 1055.]])],\n",
       " 'scores0': (tensor([0.0324, 0.0203, 0.0360,  ..., 0.0167, 0.0131, 0.0053],\n",
       "         grad_fn=<IndexBackward0>),),\n",
       " 'descriptors0': [tensor([[ 0.0043,  0.0456, -0.0140,  ..., -0.0109, -0.0232,  0.0585],\n",
       "          [-0.0728,  0.0723, -0.0682,  ..., -0.0554, -0.1620, -0.1621],\n",
       "          [-0.0619, -0.0498, -0.0140,  ...,  0.0285,  0.1492,  0.0531],\n",
       "          ...,\n",
       "          [-0.0204, -0.0708, -0.1483,  ...,  0.0700,  0.0280,  0.0339],\n",
       "          [ 0.0628,  0.1057,  0.0418,  ..., -0.0289, -0.0469,  0.0078],\n",
       "          [ 0.0849,  0.1272,  0.0585,  ...,  0.0896, -0.0176,  0.0112]],\n",
       "         grad_fn=<SelectBackward0>)]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dkm'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdkm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DKM\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mmatch_images\u001b[39m(image_path1: \u001b[38;5;28mstr\u001b[39m, image_path2: \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m      8\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124;03m    Matches features between two images using Deep Kernel Matching (DKM).\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124;03m    \u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;124;03m    :return: matched keypoints (list of tuples)\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'dkm'"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dkm import DKM\n",
    "\n",
    "def match_images(image_path1: str, image_path2: str):\n",
    "    \"\"\"\n",
    "    Matches features between two images using Deep Kernel Matching (DKM).\n",
    "    \n",
    "    :param image_path1: Path to the first image\n",
    "    :param image_path2: Path to the second image\n",
    "    :return: matched keypoints (list of tuples)\n",
    "    \"\"\"\n",
    "    # Load images\n",
    "    img1 = cv2.imread(image_path1)\n",
    "    img2 = cv2.imread(image_path2)\n",
    "    img1 = cv2.cvtColor(img1, cv2.COLOR_BGR2RGB)\n",
    "    img2 = cv2.cvtColor(img2, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Load the pretrained DKM model\n",
    "    model = DKM(pretrained=True).eval()\n",
    "    \n",
    "    # Convert images to torch tensors\n",
    "    img1_tensor = torch.from_numpy(img1).permute(2, 0, 1).float() / 255.0\n",
    "    img2_tensor = torch.from_numpy(img2).permute(2, 0, 1).float() / 255.0\n",
    "    \n",
    "    # Perform matching\n",
    "    matches, _ = model.match(img1_tensor.unsqueeze(0), img2_tensor.unsqueeze(0))\n",
    "    \n",
    "    return matches\n",
    "\n",
    "def draw_matches(image_path1: str, image_path2: str, matches):\n",
    "    \"\"\"\n",
    "    Visualizes the matches between two images.\n",
    "    \n",
    "    :param image_path1: Path to the first image\n",
    "    :param image_path2: Path to the second image\n",
    "    :param matches: Matched keypoints\n",
    "    \"\"\"\n",
    "    img1 = cv2.imread(image_path1)\n",
    "    img2 = cv2.imread(image_path2)\n",
    "    img1 = cv2.cvtColor(img1, cv2.COLOR_BGR2RGB)\n",
    "    img2 = cv2.cvtColor(img2, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Create a blank canvas to display both images side by side\n",
    "    h1, w1 = img1.shape[:2]\n",
    "    h2, w2 = img2.shape[:2]\n",
    "    canvas = np.zeros((max(h1, h2), w1 + w2, 3), dtype=np.uint8)\n",
    "    canvas[:h1, :w1, :] = img1\n",
    "    canvas[:h2, w1:w1 + w2, :] = img2\n",
    "    \n",
    "    # Draw matches\n",
    "    for (x1, y1), (x2, y2) in matches:\n",
    "        x1, y1, x2, y2 = int(x1), int(y1), int(x2) + w1, int(y2)  # Offset x2 by w1\n",
    "        cv2.circle(canvas, (x1, y1), 3, (0, 255, 0), -1)\n",
    "        cv2.circle(canvas, (x2, y2), 3, (255, 0, 0), -1)\n",
    "        cv2.line(canvas, (x1, y1), (x2, y2), (255, 255, 0), 1)\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.imshow(canvas)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "def main():\n",
    "    image1 = \"/share/project_data/test_images/trevi_fountain/00644051_242819650.jpg\"\n",
    "    image2 = \"/share/project_data/test_images/trevi_fountain/03172778_3127678804.jpg\"\n",
    "    matches = match_images(image1, image2)\n",
    "    draw_matches(image1, image2, matches)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
